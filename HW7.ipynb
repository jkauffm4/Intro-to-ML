{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkauffm4/Intro-to-ML/blob/main/HW7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wShpFtsY_nIl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqA0G7Owezx4",
        "outputId": "b1b08a96-8caa-4764-af1f-7beefb82ce91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m29V6qO6p1Mu",
        "outputId": "8ed0d593-20b6-42c7-91df-6fb200a77793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 79.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train = True, download = True)\n",
        "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zlnjiVQ5uTp"
      },
      "outputs": [],
      "source": [
        "class Q1A_Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Q1A_Net, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.pool1 = torch.nn.MaxPool2d(2)\n",
        "        self.conv2 = torch.nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = torch.nn.ReLU()\n",
        "        self.pool2 = torch.nn.MaxPool2d(2)\n",
        "        self.fc1 = torch.nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act4 = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = self.act4(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpxLFxGr6daL"
      },
      "outputs": [],
      "source": [
        "Q1A_model = Q1A_Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyMpDyVjBep0",
        "outputId": "5ad7a766-671d-4342-b000-767c22909918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([0.2470, 0.2435, 0.2616])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "cifar10 = datasets.CIFAR10(data_path, train = True, download = True)\n",
        "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True)\n",
        "img, label = cifar10[99]\n",
        "to_tensor = transforms.ToTensor()\n",
        "img_t = to_tensor(img)\n",
        "imgs = torch.stack([to_tensor(img) for img, label in cifar10], dim=3)\n",
        "imgs.view(3, -1).mean(dim=1)\n",
        "imgs.view(3, -1).std(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVnf1K4_AkwN"
      },
      "outputs": [],
      "source": [
        "transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
        "cifar10 = datasets.CIFAR10(data_path, train = True, download = False, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))\n",
        "cifar10_val = datasets.CIFAR10(data_path, train = False, download = False, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))\n",
        "img, _ = cifar10[99]\n",
        "imgs = torch.stack([img for img, _ in cifar10], dim=3)\n",
        "#imgs = imgs.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "q7gORRmg_shD",
        "outputId": "f7987927-7352-4c93-9f86-6ad93b9aeda7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 1.581403\n",
            "Epoch: 1, Loss: 1.644853\n",
            "Epoch: 2, Loss: 1.216052\n",
            "Epoch: 3, Loss: 1.664903\n",
            "Epoch: 4, Loss: 1.474846\n",
            "Epoch: 5, Loss: 1.180468\n",
            "Epoch: 6, Loss: 1.479889\n",
            "Epoch: 7, Loss: 0.966518\n",
            "Epoch: 8, Loss: 2.150442\n",
            "Epoch: 9, Loss: 1.074386\n",
            "Epoch: 10, Loss: 0.955610\n",
            "Epoch: 11, Loss: 1.238214\n",
            "Epoch: 12, Loss: 1.580573\n",
            "Epoch: 13, Loss: 1.064871\n",
            "Epoch: 14, Loss: 1.142614\n",
            "Epoch: 15, Loss: 1.455808\n",
            "Epoch: 16, Loss: 0.565571\n",
            "Epoch: 17, Loss: 1.624984\n",
            "Epoch: 18, Loss: 1.543944\n",
            "Epoch: 19, Loss: 0.650686\n",
            "Epoch: 20, Loss: 1.322194\n",
            "Epoch: 21, Loss: 1.240772\n",
            "Epoch: 22, Loss: 1.759160\n",
            "Epoch: 23, Loss: 0.643133\n",
            "Epoch: 24, Loss: 0.834033\n",
            "Epoch: 25, Loss: 1.055036\n",
            "Epoch: 26, Loss: 0.962420\n",
            "Epoch: 27, Loss: 1.308399\n",
            "Epoch: 28, Loss: 1.024362\n",
            "Epoch: 29, Loss: 1.315783\n",
            "Epoch: 30, Loss: 0.728063\n",
            "Epoch: 31, Loss: 1.304243\n",
            "Epoch: 32, Loss: 2.114763\n",
            "Epoch: 33, Loss: 1.025118\n",
            "Epoch: 34, Loss: 1.074024\n",
            "Epoch: 35, Loss: 0.915140\n",
            "Epoch: 36, Loss: 1.055834\n",
            "Epoch: 37, Loss: 1.412099\n",
            "Epoch: 38, Loss: 0.571885\n",
            "Epoch: 39, Loss: 0.985268\n",
            "Epoch: 40, Loss: 1.378166\n",
            "Epoch: 41, Loss: 0.794395\n",
            "Epoch: 42, Loss: 1.761159\n",
            "Epoch: 43, Loss: 1.141494\n",
            "Epoch: 44, Loss: 0.926452\n",
            "Epoch: 45, Loss: 1.445321\n",
            "Epoch: 46, Loss: 1.437861\n",
            "Epoch: 47, Loss: 0.938491\n",
            "Epoch: 48, Loss: 1.340610\n",
            "Epoch: 49, Loss: 0.932190\n",
            "Epoch: 50, Loss: 2.033691\n",
            "Epoch: 51, Loss: 0.988107\n",
            "Epoch: 52, Loss: 1.064305\n",
            "Epoch: 53, Loss: 1.813532\n",
            "Epoch: 54, Loss: 0.861256\n",
            "Epoch: 55, Loss: 1.293823\n",
            "Epoch: 56, Loss: 1.615167\n",
            "Epoch: 57, Loss: 1.754337\n",
            "Epoch: 58, Loss: 0.646643\n",
            "Epoch: 59, Loss: 1.372420\n",
            "Epoch: 60, Loss: 1.238772\n",
            "Epoch: 61, Loss: 0.661855\n",
            "Epoch: 62, Loss: 0.873249\n",
            "Epoch: 63, Loss: 1.148624\n",
            "Epoch: 64, Loss: 1.006900\n",
            "Epoch: 65, Loss: 1.301581\n",
            "Epoch: 66, Loss: 1.755003\n",
            "Epoch: 67, Loss: 1.823069\n",
            "Epoch: 68, Loss: 0.986068\n",
            "Epoch: 69, Loss: 1.088342\n",
            "Epoch: 70, Loss: 1.525941\n",
            "Epoch: 71, Loss: 1.239912\n",
            "Epoch: 72, Loss: 0.728976\n",
            "Epoch: 73, Loss: 1.030679\n",
            "Epoch: 74, Loss: 1.341502\n",
            "Epoch: 75, Loss: 0.938589\n",
            "Epoch: 76, Loss: 1.054148\n",
            "Epoch: 77, Loss: 1.752408\n",
            "Epoch: 78, Loss: 0.941059\n",
            "Epoch: 79, Loss: 0.962614\n",
            "Epoch: 80, Loss: 1.250927\n",
            "Epoch: 81, Loss: 0.852487\n",
            "Epoch: 82, Loss: 1.077695\n",
            "Epoch: 83, Loss: 1.605400\n",
            "Epoch: 84, Loss: 0.808416\n",
            "Epoch: 85, Loss: 0.667433\n",
            "Epoch: 86, Loss: 0.833846\n",
            "Epoch: 87, Loss: 0.858954\n",
            "Epoch: 88, Loss: 1.507665\n",
            "Epoch: 89, Loss: 1.783853\n",
            "Epoch: 90, Loss: 1.212824\n",
            "Epoch: 91, Loss: 0.912759\n",
            "Epoch: 92, Loss: 0.675445\n",
            "Epoch: 93, Loss: 1.804039\n",
            "Epoch: 94, Loss: 1.262850\n",
            "Epoch: 95, Loss: 1.282318\n",
            "Epoch: 96, Loss: 0.894910\n",
            "Epoch: 97, Loss: 1.416722\n",
            "Epoch: 98, Loss: 1.059895\n",
            "Epoch: 99, Loss: 1.082262\n",
            "Epoch: 100, Loss: 0.869912\n",
            "Epoch: 101, Loss: 1.070757\n",
            "Epoch: 102, Loss: 0.683383\n",
            "Epoch: 103, Loss: 1.159376\n",
            "Epoch: 104, Loss: 0.660643\n",
            "Epoch: 105, Loss: 1.361116\n",
            "Epoch: 106, Loss: 0.715850\n",
            "Epoch: 107, Loss: 1.510852\n",
            "Epoch: 108, Loss: 0.684258\n",
            "Epoch: 109, Loss: 1.157300\n",
            "Epoch: 110, Loss: 1.419058\n",
            "Epoch: 111, Loss: 1.341748\n",
            "Epoch: 112, Loss: 1.202692\n",
            "Epoch: 113, Loss: 0.648272\n",
            "Epoch: 114, Loss: 0.552368\n",
            "Epoch: 115, Loss: 0.823333\n",
            "Epoch: 116, Loss: 1.303084\n",
            "Epoch: 117, Loss: 1.092028\n",
            "Epoch: 118, Loss: 0.992753\n",
            "Epoch: 119, Loss: 0.818584\n",
            "Epoch: 120, Loss: 1.142085\n",
            "Epoch: 121, Loss: 2.076327\n",
            "Epoch: 122, Loss: 0.867975\n",
            "Epoch: 123, Loss: 1.028744\n",
            "Epoch: 124, Loss: 0.790673\n",
            "Epoch: 125, Loss: 1.863077\n",
            "Epoch: 126, Loss: 1.186131\n",
            "Epoch: 127, Loss: 0.934610\n",
            "Epoch: 128, Loss: 1.187727\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m----> 9\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m \u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m Q1A_model(imgs)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size = 64, shuffle = True)\n",
        "Q1A_model = Q1A_Net().to(device=device)\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(Q1A_model.parameters(), lr = learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "n_epochs = 200\n",
        "for epoch in range(n_epochs):\n",
        "  for imgs, labels in train_loader:\n",
        "    imgs = imgs.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    outputs = Q1A_model(imgs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hesWm5QMaim2",
        "outputId": "d6d14e5f-5c77-41b1-9635-b9ebe100268f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.094920\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = cifar10_model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl664mOcbLr5",
        "outputId": "4cae2e2c-cde1-4336-bf76-71ca891b62c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.093500\n"
          ]
        }
      ],
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = cifar10_model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAnC4A2U6SJ3"
      },
      "outputs": [],
      "source": [
        "class Q1B_Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Q1B_Net, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.pool1 = torch.nn.MaxPool2d(2)\n",
        "        self.conv2 = torch.nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = torch.nn.ReLU()\n",
        "        self.pool2 = torch.nn.MaxPool2d(2)\n",
        "        self.conv3 = torch.nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
        "        self.act3 = torch.nn.ReLU()\n",
        "        self.pool3 = torch.nn.MaxPool2d(2)\n",
        "        self.fc1 = torch.nn.Linear(4 * 4 * 4, 32)\n",
        "        self.act4 = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = self.pool3(self.act3(self.conv3(out)))\n",
        "        out = out.view(-1, 4 * 4 * 4)\n",
        "        out = self.act4(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "-0RerS10b81W",
        "outputId": "11ad5faf-e7cd-4b10-fbd1-cba108c135bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 2.307894\n",
            "Epoch: 1, Loss: 2.300971\n",
            "Epoch: 2, Loss: 2.298497\n",
            "Epoch: 3, Loss: 2.310263\n",
            "Epoch: 4, Loss: 2.298687\n",
            "Epoch: 5, Loss: 2.270029\n",
            "Epoch: 6, Loss: 2.308037\n",
            "Epoch: 7, Loss: 2.299601\n",
            "Epoch: 8, Loss: 2.329780\n",
            "Epoch: 9, Loss: 2.314899\n",
            "Epoch: 10, Loss: 2.305883\n",
            "Epoch: 11, Loss: 2.295322\n",
            "Epoch: 12, Loss: 2.360054\n",
            "Epoch: 13, Loss: 2.353308\n",
            "Epoch: 14, Loss: 2.270508\n",
            "Epoch: 15, Loss: 2.335371\n",
            "Epoch: 16, Loss: 2.259885\n",
            "Epoch: 17, Loss: 2.288702\n",
            "Epoch: 18, Loss: 2.315046\n",
            "Epoch: 19, Loss: 2.290932\n",
            "Epoch: 20, Loss: 2.355801\n",
            "Epoch: 21, Loss: 2.309774\n",
            "Epoch: 22, Loss: 2.277247\n",
            "Epoch: 23, Loss: 2.280517\n",
            "Epoch: 24, Loss: 2.321770\n",
            "Epoch: 25, Loss: 2.341521\n",
            "Epoch: 26, Loss: 2.345055\n",
            "Epoch: 27, Loss: 2.328620\n",
            "Epoch: 28, Loss: 2.298835\n",
            "Epoch: 29, Loss: 2.286715\n",
            "Epoch: 30, Loss: 2.250443\n",
            "Epoch: 31, Loss: 2.260070\n",
            "Epoch: 32, Loss: 2.298206\n",
            "Epoch: 33, Loss: 2.312478\n",
            "Epoch: 34, Loss: 2.346912\n",
            "Epoch: 35, Loss: 2.324882\n",
            "Epoch: 36, Loss: 2.339622\n",
            "Epoch: 37, Loss: 2.280136\n",
            "Epoch: 38, Loss: 2.248077\n",
            "Epoch: 39, Loss: 2.253320\n",
            "Epoch: 40, Loss: 2.324932\n",
            "Epoch: 41, Loss: 2.309541\n",
            "Epoch: 42, Loss: 2.294962\n",
            "Epoch: 43, Loss: 2.267489\n",
            "Epoch: 44, Loss: 2.298911\n",
            "Epoch: 45, Loss: 2.313805\n",
            "Epoch: 46, Loss: 2.302215\n",
            "Epoch: 47, Loss: 2.279872\n",
            "Epoch: 48, Loss: 2.296957\n",
            "Epoch: 49, Loss: 2.296349\n",
            "Epoch: 50, Loss: 2.306044\n",
            "Epoch: 51, Loss: 2.297889\n",
            "Epoch: 52, Loss: 2.324938\n",
            "Epoch: 53, Loss: 2.284393\n",
            "Epoch: 54, Loss: 2.272386\n",
            "Epoch: 55, Loss: 2.266125\n",
            "Epoch: 56, Loss: 2.280460\n",
            "Epoch: 57, Loss: 2.272693\n",
            "Epoch: 58, Loss: 2.298071\n",
            "Epoch: 59, Loss: 2.301153\n",
            "Epoch: 60, Loss: 2.326043\n",
            "Epoch: 61, Loss: 2.282387\n",
            "Epoch: 62, Loss: 2.299772\n",
            "Epoch: 63, Loss: 2.324013\n",
            "Epoch: 64, Loss: 2.312179\n",
            "Epoch: 65, Loss: 2.315309\n",
            "Epoch: 66, Loss: 2.281401\n",
            "Epoch: 67, Loss: 2.313317\n",
            "Epoch: 68, Loss: 2.314810\n",
            "Epoch: 69, Loss: 2.303299\n",
            "Epoch: 70, Loss: 2.355350\n",
            "Epoch: 71, Loss: 2.315287\n",
            "Epoch: 72, Loss: 2.293341\n",
            "Epoch: 73, Loss: 2.312089\n",
            "Epoch: 74, Loss: 2.273235\n",
            "Epoch: 75, Loss: 2.310520\n",
            "Epoch: 76, Loss: 2.313421\n",
            "Epoch: 77, Loss: 2.314963\n",
            "Epoch: 78, Loss: 2.308673\n",
            "Epoch: 79, Loss: 2.354145\n",
            "Epoch: 80, Loss: 2.275680\n",
            "Epoch: 81, Loss: 2.324916\n",
            "Epoch: 82, Loss: 2.322238\n",
            "Epoch: 83, Loss: 2.373914\n",
            "Epoch: 84, Loss: 2.324721\n",
            "Epoch: 85, Loss: 2.294511\n",
            "Epoch: 86, Loss: 2.353465\n",
            "Epoch: 87, Loss: 2.354018\n",
            "Epoch: 88, Loss: 2.298722\n",
            "Epoch: 89, Loss: 2.255407\n",
            "Epoch: 90, Loss: 2.299062\n",
            "Epoch: 91, Loss: 2.323379\n",
            "Epoch: 92, Loss: 2.364245\n",
            "Epoch: 93, Loss: 2.307345\n",
            "Epoch: 94, Loss: 2.346769\n",
            "Epoch: 95, Loss: 2.293301\n",
            "Epoch: 96, Loss: 2.350131\n",
            "Epoch: 97, Loss: 2.300709\n",
            "Epoch: 98, Loss: 2.311592\n",
            "Epoch: 99, Loss: 2.299313\n",
            "Epoch: 100, Loss: 2.313869\n",
            "Epoch: 101, Loss: 2.320896\n",
            "Epoch: 102, Loss: 2.310267\n",
            "Epoch: 103, Loss: 2.304084\n",
            "Epoch: 104, Loss: 2.302955\n",
            "Epoch: 105, Loss: 2.260224\n",
            "Epoch: 106, Loss: 2.317600\n",
            "Epoch: 107, Loss: 2.293028\n",
            "Epoch: 108, Loss: 2.312051\n",
            "Epoch: 109, Loss: 2.353158\n",
            "Epoch: 110, Loss: 2.329490\n",
            "Epoch: 111, Loss: 2.280111\n",
            "Epoch: 112, Loss: 2.304118\n",
            "Epoch: 113, Loss: 2.308771\n",
            "Epoch: 114, Loss: 2.346166\n",
            "Epoch: 115, Loss: 2.250563\n",
            "Epoch: 116, Loss: 2.318277\n",
            "Epoch: 117, Loss: 2.333880\n",
            "Epoch: 118, Loss: 2.313806\n",
            "Epoch: 119, Loss: 2.332508\n",
            "Epoch: 120, Loss: 2.312732\n",
            "Epoch: 121, Loss: 2.297120\n",
            "Epoch: 122, Loss: 2.322640\n",
            "Epoch: 123, Loss: 2.366036\n",
            "Epoch: 124, Loss: 2.318993\n",
            "Epoch: 125, Loss: 2.287817\n",
            "Epoch: 126, Loss: 2.336340\n",
            "Epoch: 127, Loss: 2.292228\n",
            "Epoch: 128, Loss: 2.321082\n",
            "Epoch: 129, Loss: 2.326184\n",
            "Epoch: 130, Loss: 2.276793\n",
            "Epoch: 131, Loss: 2.295404\n",
            "Epoch: 132, Loss: 2.333519\n",
            "Epoch: 133, Loss: 2.293215\n",
            "Epoch: 134, Loss: 2.324774\n",
            "Epoch: 135, Loss: 2.298740\n",
            "Epoch: 136, Loss: 2.302612\n",
            "Epoch: 137, Loss: 2.319620\n",
            "Epoch: 138, Loss: 2.257386\n",
            "Epoch: 139, Loss: 2.322772\n",
            "Epoch: 140, Loss: 2.325462\n",
            "Epoch: 141, Loss: 2.247251\n",
            "Epoch: 142, Loss: 2.306926\n",
            "Epoch: 143, Loss: 2.330644\n",
            "Epoch: 144, Loss: 2.334821\n",
            "Epoch: 145, Loss: 2.333398\n",
            "Epoch: 146, Loss: 2.313557\n",
            "Epoch: 147, Loss: 2.308555\n",
            "Epoch: 148, Loss: 2.253777\n",
            "Epoch: 149, Loss: 2.279531\n",
            "Epoch: 150, Loss: 2.296184\n",
            "Epoch: 151, Loss: 2.305327\n",
            "Epoch: 152, Loss: 2.311231\n",
            "Epoch: 153, Loss: 2.352771\n",
            "Epoch: 154, Loss: 2.308209\n",
            "Epoch: 155, Loss: 2.282323\n",
            "Epoch: 156, Loss: 2.330228\n",
            "Epoch: 157, Loss: 2.329225\n",
            "Epoch: 158, Loss: 2.310042\n",
            "Epoch: 159, Loss: 2.270468\n",
            "Epoch: 160, Loss: 2.315789\n",
            "Epoch: 161, Loss: 2.365788\n",
            "Epoch: 162, Loss: 2.323595\n",
            "Epoch: 163, Loss: 2.299602\n",
            "Epoch: 164, Loss: 2.295847\n",
            "Epoch: 165, Loss: 2.256234\n",
            "Epoch: 166, Loss: 2.301018\n",
            "Epoch: 167, Loss: 2.322334\n",
            "Epoch: 168, Loss: 2.328115\n",
            "Epoch: 169, Loss: 2.316182\n",
            "Epoch: 170, Loss: 2.273448\n",
            "Epoch: 171, Loss: 2.293081\n",
            "Epoch: 172, Loss: 2.305273\n",
            "Epoch: 173, Loss: 2.288751\n",
            "Epoch: 174, Loss: 2.323083\n",
            "Epoch: 175, Loss: 2.304461\n",
            "Epoch: 176, Loss: 2.298143\n",
            "Epoch: 177, Loss: 2.284254\n",
            "Epoch: 178, Loss: 2.328946\n",
            "Epoch: 179, Loss: 2.314623\n",
            "Epoch: 180, Loss: 2.354272\n",
            "Epoch: 181, Loss: 2.297348\n",
            "Epoch: 182, Loss: 2.281846\n",
            "Epoch: 183, Loss: 2.287410\n",
            "Epoch: 184, Loss: 2.333922\n",
            "Epoch: 185, Loss: 2.325597\n",
            "Epoch: 186, Loss: 2.322795\n",
            "Epoch: 187, Loss: 2.300669\n",
            "Epoch: 188, Loss: 2.322137\n",
            "Epoch: 189, Loss: 2.330844\n",
            "Epoch: 190, Loss: 2.294280\n",
            "Epoch: 191, Loss: 2.312740\n",
            "Epoch: 192, Loss: 2.316294\n",
            "Epoch: 193, Loss: 2.258672\n",
            "Epoch: 194, Loss: 2.331213\n",
            "Epoch: 195, Loss: 2.330427\n",
            "Epoch: 196, Loss: 2.309705\n",
            "Epoch: 197, Loss: 2.283217\n",
            "Epoch: 198, Loss: 2.287800\n",
            "Epoch: 199, Loss: 2.285424\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size = 64, shuffle = True)\n",
        "cifar10_model = Q1B_Net()\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(cifar10_model.parameters(), lr = learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "n_epochs = 200\n",
        "for epoch in range(n_epochs):\n",
        "  for imgs, labels in train_loader:\n",
        "    outputs = cifar10_model(imgs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "0riya_mk8g-9",
        "outputId": "d68dfc27-457a-4707-a83c-0f737bf97260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.099820\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = cifar10_model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldall5F5ezx_",
        "outputId": "7bd70390-d3dc-4ba8-94f3-2efa5cd0487f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.100100\n"
          ]
        }
      ],
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = cifar10_model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en03GaZdezyA"
      },
      "outputs": [],
      "source": [
        "class ResNetBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = torch.nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "                torch.nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        out = self.act1(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.nn.ReLU()(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNiBP578ezyA"
      },
      "outputs": [],
      "source": [
        "class Q2A_Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Q2A_Net, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "\n",
        "        self.resnet1 = ResNetBlock(16, 16)\n",
        "        self.resnet2 = ResNetBlock(16, 16)\n",
        "        self.resnet3 = ResNetBlock(16, 32)\n",
        "        self.resnet4 = ResNetBlock(32, 32)\n",
        "        self.resnet5 = ResNetBlock(32, 64)\n",
        "        self.resnet6 = ResNetBlock(64, 64)\n",
        "        self.resnet7 = ResNetBlock(64, 128)\n",
        "        self.resnet8 = ResNetBlock(128, 128)\n",
        "        self.resnet9 = ResNetBlock(128, 256)\n",
        "        self.resnet10 = ResNetBlock(256, 256)\n",
        "\n",
        "        self.pool = torch.nn.MaxPool2d(2)\n",
        "        self.fc1 = torch.nn.Linear(256 * 2 * 2, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.act1(self.bn1(self.conv1(x)))\n",
        "        out = self.pool(out)\n",
        "\n",
        "        out = self.resnet1(out)\n",
        "        out = self.pool(out)\n",
        "        out = self.resnet2(out)\n",
        "        out = self.pool(out)\n",
        "        out = self.resnet3(out)\n",
        "        out = self.pool(out)\n",
        "        out = self.resnet4(out)\n",
        "      #  out = self.pool(out)\n",
        "        out = self.resnet5(out)\n",
        "     #   out = self.pool(out)\n",
        "        out = self.resnet6(out)\n",
        "      #  out = self.pool(out)\n",
        "        out = self.resnet7(out)\n",
        "       # out = self.pool(out)\n",
        "        out = self.resnet8(out)\n",
        "      #  out = self.pool(out)\n",
        "        out = self.resnet9(out)\n",
        "      #  out = self.pool(out)\n",
        "        out = self.resnet10(out)\n",
        "\n",
        "        out = out.view(-1, 256 * 2 * 2)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VMuHUBHZezyA",
        "outputId": "3e45d9ed-00c7-42ff-bc1d-351480c312d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 1.630592\n",
            "Epoch: 1, Loss: 0.916337\n",
            "Epoch: 2, Loss: 2.246473\n",
            "Epoch: 3, Loss: 0.932922\n",
            "Epoch: 4, Loss: 0.989297\n",
            "Epoch: 5, Loss: 0.597788\n",
            "Epoch: 6, Loss: 1.016242\n",
            "Epoch: 7, Loss: 0.729669\n",
            "Epoch: 8, Loss: 0.157600\n",
            "Epoch: 9, Loss: 1.026040\n",
            "Epoch: 10, Loss: 0.771464\n",
            "Epoch: 11, Loss: 0.346271\n",
            "Epoch: 12, Loss: 0.615090\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size = 64, shuffle = True)\n",
        "Q2A_model = Q2A_Net().to(device=device)\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(Q2A_model.parameters(), lr = learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "n_epochs = 200\n",
        "for epoch in range(n_epochs):\n",
        "  for imgs, labels in train_loader:\n",
        "    imgs = imgs.to(device=device)\n",
        "    labels = labels.to(device=device)\n",
        "    outputs = Q2A_model(imgs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        outputs = Q2A_model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "id": "xdLDQjuvrYQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs = imgs.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        outputs = Q2A_model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "id": "RqK0z9icras8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}