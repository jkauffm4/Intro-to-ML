{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYh9WuEvV9/zXGWPWfJ+i9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkauffm4/Intro-to-ML/blob/main/HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wShpFtsY_nIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270952b9-b026-4133-fbd5-810bbe734dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_map(x):\n",
        "  return x.map({'yes': 1, 'no': 0})"
      ],
      "metadata": {
        "id": "Equ87R3L_0u3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "file_path = '/content/drive/My Drive/Intro-to-ML/Housing.csv'\n",
        "sample = pd.DataFrame(pd.read_csv(file_path))\n",
        "varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "sample[varlist] = sample[varlist].apply(binary_map)\n",
        "np.random.seed(0)\n",
        "train_set, test_set = train_test_split(sample, train_size = .8, test_size = .2, random_state = 100)\n",
        "Y = train_set.pop('price')\n",
        "Y_test = test_set.pop('price')\n",
        "junk = train_set.pop('furnishingstatus') #Gotta remove this, its not used\n",
        "junk = test_set.pop('furnishingstatus') #Same with this\n",
        "blerg = train_set\n",
        "train_set_all = train_set.to_numpy()\n",
        "test_set_all = test_set.to_numpy()\n",
        "# Theres a better way to get out the values here, but I don't know it\n",
        "junk = train_set.pop('mainroad')\n",
        "junk = train_set.pop('guestroom')\n",
        "junk = train_set.pop('hotwaterheating')\n",
        "junk = train_set.pop('basement')\n",
        "junk = train_set.pop('airconditioning')\n",
        "junk = train_set.pop('prefarea')\n",
        "junk = test_set.pop('mainroad')\n",
        "junk = test_set.pop('guestroom')\n",
        "junk = test_set.pop('hotwaterheating')\n",
        "junk = test_set.pop('basement')\n",
        "junk = test_set.pop('airconditioning')\n",
        "junk = test_set.pop('prefarea')\n",
        "X_train = train_set.to_numpy()\n",
        "X_test = test_set.to_numpy()\n",
        "Y = Y.to_numpy()\n",
        "Y_test = Y_test.to_numpy()\n",
        "size = len(X_train)\n",
        "x0 = np.ones((size,1))"
      ],
      "metadata": {
        "id": "uOZkgN1f_6NR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_x, val_x, train_y, val_y):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "    pred_y = model(train_x)\n",
        "    train_loss = loss_fn(pred_y, train_y)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_pred_y = model(val_x)\n",
        "      val_loss = loss_fn(val_pred_y, val_y)\n",
        "      assert val_loss.requires_grad == False\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
        "            f\" Validation loss {val_loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "h1yO_0qwO5mX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_standard_test = StandardScaler().fit_transform(test_set_all)\n",
        "X_standard_train = StandardScaler().fit_transform(train_set_all)\n",
        "train_x = torch.tensor(X_standard_train, dtype=torch.float32)\n",
        "train_y = torch.tensor(Y, dtype=torch.float32)\n",
        "val_x = torch.tensor(X_standard_test, dtype=torch.float32)\n",
        "val_y = torch.tensor(Y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "i6TGfJNFJFDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_nn = torch.nn.Sequential(\n",
        "    torch.nn.Linear(11,8),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(8,1)\n",
        ")"
      ],
      "metadata": {
        "id": "4B78YxTQJJgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(housing_nn.parameters(), lr = 1e-1)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "training_loop(n_epochs = 5000,\n",
        "              optimizer = optimizer,\n",
        "              model = housing_nn,\n",
        "              loss_fn = loss_fn,\n",
        "              train_x = train_x,\n",
        "              train_y = train_y,\n",
        "              val_x = val_x,\n",
        "              val_y = val_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oafZbqHXJ9uE",
        "outputId": "3fd459bd-9ff1-4e27-cfcf-575dec7c4f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([436])) that is different to the input size (torch.Size([436, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([109])) that is different to the input size (torch.Size([109, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training loss 26424480104448.0000, Validation loss 25106363449344.0000\n",
            "Epoch 1000, Training loss 26420409532416.0000, Validation loss 25102349500416.0000\n",
            "Epoch 1500, Training loss 26416341057536.0000, Validation loss 25098333454336.0000\n",
            "Epoch 2000, Training loss 26412264194048.0000, Validation loss 25094313213952.0000\n",
            "Epoch 2500, Training loss 26408193622016.0000, Validation loss 25090295070720.0000\n",
            "Epoch 3000, Training loss 26404120952832.0000, Validation loss 25086281121792.0000\n",
            "Epoch 3500, Training loss 26400054575104.0000, Validation loss 25082267172864.0000\n",
            "Epoch 4000, Training loss 26395984003072.0000, Validation loss 25078253223936.0000\n",
            "Epoch 4500, Training loss 26391911333888.0000, Validation loss 25074237177856.0000\n",
            "Epoch 5000, Training loss 26387838664704.0000, Validation loss 25070219034624.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing_nn_extended = torch.nn.Sequential(\n",
        "    torch.nn.Linear(11, 11),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(11,8),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(8,5),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(5,1)\n",
        ")"
      ],
      "metadata": {
        "id": "lVIdEk1-LSNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing_optimizer_extended = torch.optim.Adam(housing_nn_extended.parameters(), lr = 1e-1)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "training_loop(n_epochs = 5000,\n",
        "              optimizer = housing_optimizer_extended,\n",
        "              model = housing_nn_extended,\n",
        "              loss_fn = loss_fn,\n",
        "              train_x = train_x,\n",
        "              train_y = train_y,\n",
        "              val_x = val_x,\n",
        "              val_y = val_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pCkZPpDLuMr",
        "outputId": "ea1bc45b-de78-4b4b-af59-272e1d7d7a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([436])) that is different to the input size (torch.Size([436, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([109])) that is different to the input size (torch.Size([109, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training loss 26474708992000.0000, Validation loss 25155961094144.0000\n",
            "Epoch 1000, Training loss 26471829602304.0000, Validation loss 25153167687680.0000\n",
            "Epoch 1500, Training loss 26468948115456.0000, Validation loss 25150374281216.0000\n",
            "Epoch 2000, Training loss 26466072920064.0000, Validation loss 25147587166208.0000\n",
            "Epoch 2500, Training loss 26463195627520.0000, Validation loss 25144795856896.0000\n",
            "Epoch 3000, Training loss 26460320432128.0000, Validation loss 25142004547584.0000\n",
            "Epoch 3500, Training loss 26457443139584.0000, Validation loss 25139217432576.0000\n",
            "Epoch 4000, Training loss 26454565847040.0000, Validation loss 25136428220416.0000\n",
            "Epoch 4500, Training loss 26451694845952.0000, Validation loss 25133641105408.0000\n",
            "Epoch 5000, Training loss 26448819650560.0000, Validation loss 25130853990400.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast = load_breast_cancer()\n",
        "breast_data = breast.data\n",
        "breast_labels = breast.target\n",
        "labels = np.reshape(breast_labels,(569,1))\n",
        "final_breast_data = np.concatenate([breast_data,labels],axis=1)\n",
        "breast_dataset = pd.DataFrame(final_breast_data)\n",
        "features = breast.feature_names\n",
        "features_labels = np.append(features, 'label')\n",
        "breast_dataset.columns = features_labels\n",
        "X_breast_data = breast_dataset.iloc[0:569,0:29].values\n",
        "Y_breast_data = breast_dataset.iloc[:,30].values\n",
        "sscale = StandardScaler()\n",
        "X_breast_standardized = sscale.fit_transform(X_breast_data)\n",
        "X_breast_train, X_breast_test, Y_breast_train, Y_breast_test = train_test_split(X_breast_standardized, Y_breast_data, train_size = .8, test_size = .2, random_state = 100)\n",
        "X_breast_train = torch.tensor(X_breast_train, dtype=torch.float32)\n",
        "X_breast_test = torch.tensor(X_breast_test, dtype=torch.float32)\n",
        "Y_breast_train = torch.tensor(Y_breast_train, dtype=torch.float32)\n",
        "Y_breast_test = torch.tensor(Y_breast_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "f6K4IrDbSCwS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_nn = torch.nn.Sequential(\n",
        "    torch.nn.Linear(29,32),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(32,1)\n",
        ")"
      ],
      "metadata": {
        "id": "HzlirVqbVK6U"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_optimizer = torch.optim.Adam(cancer_nn.parameters(), lr = 1e-1)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "training_loop(n_epochs = 5000,\n",
        "              optimizer = cancer_optimizer,\n",
        "              model = cancer_nn,\n",
        "              loss_fn = loss_fn,\n",
        "              train_x = X_breast_train,\n",
        "              train_y = Y_breast_train,\n",
        "              val_x = X_breast_test,\n",
        "              val_y = Y_breast_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVpVpPYbWUoW",
        "outputId": "b04fb65b-f41f-41c4-c7c3-5ed4682b60f0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([455])) that is different to the input size (torch.Size([455, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([114])) that is different to the input size (torch.Size([114, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training loss 0.2300, Validation loss 0.2502\n",
            "Epoch 1000, Training loss 0.2299, Validation loss 0.2503\n",
            "Epoch 1500, Training loss 0.2299, Validation loss 0.2502\n",
            "Epoch 2000, Training loss 0.2299, Validation loss 0.2505\n",
            "Epoch 2500, Training loss 0.2299, Validation loss 0.2503\n",
            "Epoch 3000, Training loss 0.2299, Validation loss 0.2503\n",
            "Epoch 3500, Training loss 0.2299, Validation loss 0.2502\n",
            "Epoch 4000, Training loss 0.2299, Validation loss 0.2502\n",
            "Epoch 4500, Training loss 0.2375, Validation loss 0.2617\n",
            "Epoch 5000, Training loss 0.2299, Validation loss 0.2502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_nn_extended = torch.nn.Sequential(\n",
        "    torch.nn.Linear(29, 32),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(32,16),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(16,8),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(8,1)\n",
        ")"
      ],
      "metadata": {
        "id": "iUc0MEq9ao1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_optimizer_extended = torch.optim.Adam(cancer_nn_extended.parameters(), lr = 1e-1)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "training_loop(n_epochs = 5000,\n",
        "              optimizer = cancer_optimizer_extended,\n",
        "              model = cancer_nn_extended,\n",
        "              loss_fn = loss_fn,\n",
        "              train_x = X_breast_train,\n",
        "              train_y = Y_breast_train,\n",
        "              val_x = X_breast_test,\n",
        "              val_y = Y_breast_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUwfKJaEbD70",
        "outputId": "907d0856-ba34-4da8-e42d-c72e66e5168f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([455])) that is different to the input size (torch.Size([455, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([114])) that is different to the input size (torch.Size([114, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500, Training loss 0.2299, Validation loss 0.2502\n",
            "Epoch 1000, Training loss 0.2299, Validation loss 0.2502\n",
            "Epoch 1500, Training loss 0.2299, Validation loss 0.2502\n",
            "Epoch 2000, Training loss 0.2303, Validation loss 0.2511\n",
            "Epoch 2500, Training loss 0.2299, Validation loss 0.2502\n",
            "Epoch 3000, Training loss 0.2299, Validation loss 0.2505\n",
            "Epoch 3500, Training loss 0.2314, Validation loss 0.2511\n",
            "Epoch 4000, Training loss 0.2299, Validation loss 0.2543\n",
            "Epoch 4500, Training loss 0.2299, Validation loss 0.2533\n",
            "Epoch 5000, Training loss 0.2299, Validation loss 0.2519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train = True, download = True)\n",
        "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m29V6qO6p1Mu",
        "outputId": "06631765-ecff-466a-d53a-a052c625f024"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data-unversioned/p1ch7/cifar-10-python.tar.gz to ../data-unversioned/p1ch7/\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "img, label = cifar10[99]\n",
        "to_tensor = transforms.ToTensor()\n",
        "img_t = to_tensor(img)\n",
        "imgs = torch.stack([to_tensor(img) for img, label in cifar10], dim=3)\n",
        "imgs.view(3, -1).mean(dim=1)\n",
        "imgs.view(3, -1).std(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-SOo4eCtAws",
        "outputId": "eb0abd5e-2054-4d0d-e49c-d6510265361a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2470, 0.2435, 0.2616])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n",
        "cifar10 = datasets.CIFAR10(data_path, train = True, download = False, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))\n",
        "cifar10_val = datasets.CIFAR10(data_path, train = False, download = False, transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))]))"
      ],
      "metadata": {
        "id": "x75LZTMFuw-H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum()"
      ],
      "metadata": {
        "id": "-LC8Fi3Wl6r8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size = 64, shuffle = True)\n",
        "cifar10_model = torch.nn.Sequential(torch.nn.Linear(3072, 256), torch.nn.Tanh(), torch.nn.Linear(256, 10), torch.nn.LogSoftmax(dim = 1))\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(cifar10_model.parameters(), lr = learning_rate)\n",
        "loss_fn = torch.nn.NLLLoss()\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "  for imgs, labels in train_loader:\n",
        "    batch_size = imgs.shape[0]\n",
        "    outputs = cifar10_model(imgs.view(batch_size, -1))\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuMc-6DLoepX",
        "outputId": "71bbc8f9-cddc-4189-836b-422a484bd2af"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.323144\n",
            "Epoch: 1, Loss: 2.296948\n",
            "Epoch: 2, Loss: 2.345542\n",
            "Epoch: 3, Loss: 2.297603\n",
            "Epoch: 4, Loss: 2.398983\n",
            "Epoch: 5, Loss: 2.325710\n",
            "Epoch: 6, Loss: 2.273215\n",
            "Epoch: 7, Loss: 2.542344\n",
            "Epoch: 8, Loss: 2.423949\n",
            "Epoch: 9, Loss: 2.339843\n",
            "Epoch: 10, Loss: 2.376061\n",
            "Epoch: 11, Loss: 2.449663\n",
            "Epoch: 12, Loss: 2.387151\n",
            "Epoch: 13, Loss: 2.437116\n",
            "Epoch: 14, Loss: 2.415786\n",
            "Epoch: 15, Loss: 2.354473\n",
            "Epoch: 16, Loss: 2.392390\n",
            "Epoch: 17, Loss: 2.364003\n",
            "Epoch: 18, Loss: 2.481707\n",
            "Epoch: 19, Loss: 2.338970\n",
            "Epoch: 20, Loss: 2.300961\n",
            "Epoch: 21, Loss: 2.476236\n",
            "Epoch: 22, Loss: 2.290874\n",
            "Epoch: 23, Loss: 2.339228\n",
            "Epoch: 24, Loss: 2.362416\n",
            "Epoch: 25, Loss: 2.370901\n",
            "Epoch: 26, Loss: 2.294543\n",
            "Epoch: 27, Loss: 2.310260\n",
            "Epoch: 28, Loss: 2.431381\n",
            "Epoch: 29, Loss: 2.478729\n",
            "Epoch: 30, Loss: 2.385786\n",
            "Epoch: 31, Loss: 2.369170\n",
            "Epoch: 32, Loss: 2.375442\n",
            "Epoch: 33, Loss: 2.395299\n",
            "Epoch: 34, Loss: 2.335254\n",
            "Epoch: 35, Loss: 2.286980\n",
            "Epoch: 36, Loss: 2.270180\n",
            "Epoch: 37, Loss: 2.383752\n",
            "Epoch: 38, Loss: 2.502909\n",
            "Epoch: 39, Loss: 2.346761\n",
            "Epoch: 40, Loss: 2.466230\n",
            "Epoch: 41, Loss: 2.385639\n",
            "Epoch: 42, Loss: 2.285733\n",
            "Epoch: 43, Loss: 2.424157\n",
            "Epoch: 44, Loss: 2.354737\n",
            "Epoch: 45, Loss: 2.337017\n",
            "Epoch: 46, Loss: 2.420695\n",
            "Epoch: 47, Loss: 2.426119\n",
            "Epoch: 48, Loss: 2.465098\n",
            "Epoch: 49, Loss: 2.369272\n",
            "Epoch: 50, Loss: 2.463883\n",
            "Epoch: 51, Loss: 2.312645\n",
            "Epoch: 52, Loss: 2.447215\n",
            "Epoch: 53, Loss: 2.377639\n",
            "Epoch: 54, Loss: 2.377239\n",
            "Epoch: 55, Loss: 2.377854\n",
            "Epoch: 56, Loss: 2.452373\n",
            "Epoch: 57, Loss: 2.332438\n",
            "Epoch: 58, Loss: 2.554143\n",
            "Epoch: 59, Loss: 2.410150\n",
            "Epoch: 60, Loss: 2.317964\n",
            "Epoch: 61, Loss: 2.320081\n",
            "Epoch: 62, Loss: 2.415156\n",
            "Epoch: 63, Loss: 2.441605\n",
            "Epoch: 64, Loss: 2.479218\n",
            "Epoch: 65, Loss: 2.451242\n",
            "Epoch: 66, Loss: 2.377038\n",
            "Epoch: 67, Loss: 2.383877\n",
            "Epoch: 68, Loss: 2.470206\n",
            "Epoch: 69, Loss: 2.323868\n",
            "Epoch: 70, Loss: 2.338376\n",
            "Epoch: 71, Loss: 2.266384\n",
            "Epoch: 72, Loss: 2.440339\n",
            "Epoch: 73, Loss: 2.521327\n",
            "Epoch: 74, Loss: 2.371595\n",
            "Epoch: 75, Loss: 2.268954\n",
            "Epoch: 76, Loss: 2.327841\n",
            "Epoch: 77, Loss: 2.348928\n",
            "Epoch: 78, Loss: 2.384172\n",
            "Epoch: 79, Loss: 2.428845\n",
            "Epoch: 80, Loss: 2.449823\n",
            "Epoch: 81, Loss: 2.357885\n",
            "Epoch: 82, Loss: 2.331437\n",
            "Epoch: 83, Loss: 2.340657\n",
            "Epoch: 84, Loss: 2.415248\n",
            "Epoch: 85, Loss: 2.349874\n",
            "Epoch: 86, Loss: 2.411581\n",
            "Epoch: 87, Loss: 2.406986\n",
            "Epoch: 88, Loss: 2.350403\n",
            "Epoch: 89, Loss: 2.399871\n",
            "Epoch: 90, Loss: 2.393303\n",
            "Epoch: 91, Loss: 2.345011\n",
            "Epoch: 92, Loss: 2.414254\n",
            "Epoch: 93, Loss: 2.432624\n",
            "Epoch: 94, Loss: 2.349565\n",
            "Epoch: 95, Loss: 2.416957\n",
            "Epoch: 96, Loss: 2.399983\n",
            "Epoch: 97, Loss: 2.469936\n",
            "Epoch: 98, Loss: 2.366231\n",
            "Epoch: 99, Loss: 2.414398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = cifar10_model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNUTy8T-Db1-",
        "outputId": "d7e04c46-6e78-4d7e-ef82-118ac0dabf83"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.074020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size = 64, shuffle = True)\n",
        "cifar10_model_extended = torch.nn.Sequential(torch.nn.Linear(3072, 1024), torch.nn.Tanh(), torch.nn.Linear(1024, 256), torch.nn.Tanh(), torch.nn.Linear(256, 64), torch.nn.Tanh(), torch.nn.Linear(64,10), torch.nn.LogSoftmax(dim = 1))\n",
        "learning_rate = 1e-2\n",
        "optimizer = torch.optim.Adam(cifar10_model_extended.parameters(), lr = learning_rate)\n",
        "loss_fn = torch.nn.NLLLoss()\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "  for imgs, labels in train_loader:\n",
        "    batch_size = imgs.shape[0]\n",
        "    outputs = cifar10_model(imgs.view(imgs.shape[0], -1))\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward\n",
        "    optimizer.step()\n",
        "  print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpT4qNDLKWZN",
        "outputId": "747f5e7a-8da2-434d-fe3c-bb773c74b825"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.273928\n",
            "Epoch: 1, Loss: 2.411807\n",
            "Epoch: 2, Loss: 2.376977\n",
            "Epoch: 3, Loss: 2.428854\n",
            "Epoch: 4, Loss: 2.459967\n",
            "Epoch: 5, Loss: 2.335244\n",
            "Epoch: 6, Loss: 2.393790\n",
            "Epoch: 7, Loss: 2.468582\n",
            "Epoch: 8, Loss: 2.328747\n",
            "Epoch: 9, Loss: 2.256191\n",
            "Epoch: 10, Loss: 2.355838\n",
            "Epoch: 11, Loss: 2.366473\n",
            "Epoch: 12, Loss: 2.419211\n",
            "Epoch: 13, Loss: 2.311019\n",
            "Epoch: 14, Loss: 2.377895\n",
            "Epoch: 15, Loss: 2.313069\n",
            "Epoch: 16, Loss: 2.402389\n",
            "Epoch: 17, Loss: 2.316300\n",
            "Epoch: 18, Loss: 2.319998\n",
            "Epoch: 19, Loss: 2.473073\n",
            "Epoch: 20, Loss: 2.314267\n",
            "Epoch: 21, Loss: 2.371730\n",
            "Epoch: 22, Loss: 2.324185\n",
            "Epoch: 23, Loss: 2.338358\n",
            "Epoch: 24, Loss: 2.450111\n",
            "Epoch: 25, Loss: 2.439771\n",
            "Epoch: 26, Loss: 2.331348\n",
            "Epoch: 27, Loss: 2.385567\n",
            "Epoch: 28, Loss: 2.493401\n",
            "Epoch: 29, Loss: 2.419688\n",
            "Epoch: 30, Loss: 2.296521\n",
            "Epoch: 31, Loss: 2.422075\n",
            "Epoch: 32, Loss: 2.437207\n",
            "Epoch: 33, Loss: 2.455475\n",
            "Epoch: 34, Loss: 2.330341\n",
            "Epoch: 35, Loss: 2.322149\n",
            "Epoch: 36, Loss: 2.273469\n",
            "Epoch: 37, Loss: 2.466022\n",
            "Epoch: 38, Loss: 2.336422\n",
            "Epoch: 39, Loss: 2.441935\n",
            "Epoch: 40, Loss: 2.386059\n",
            "Epoch: 41, Loss: 2.232008\n",
            "Epoch: 42, Loss: 2.322664\n",
            "Epoch: 43, Loss: 2.428710\n",
            "Epoch: 44, Loss: 2.410259\n",
            "Epoch: 45, Loss: 2.323110\n",
            "Epoch: 46, Loss: 2.365026\n",
            "Epoch: 47, Loss: 2.373578\n",
            "Epoch: 48, Loss: 2.394320\n",
            "Epoch: 49, Loss: 2.331551\n",
            "Epoch: 50, Loss: 2.233453\n",
            "Epoch: 51, Loss: 2.313979\n",
            "Epoch: 52, Loss: 2.373664\n",
            "Epoch: 53, Loss: 2.380556\n",
            "Epoch: 54, Loss: 2.368493\n",
            "Epoch: 55, Loss: 2.399572\n",
            "Epoch: 56, Loss: 2.389096\n",
            "Epoch: 57, Loss: 2.364028\n",
            "Epoch: 58, Loss: 2.400271\n",
            "Epoch: 59, Loss: 2.389607\n",
            "Epoch: 60, Loss: 2.363002\n",
            "Epoch: 61, Loss: 2.264148\n",
            "Epoch: 62, Loss: 2.505545\n",
            "Epoch: 63, Loss: 2.454277\n",
            "Epoch: 64, Loss: 2.325422\n",
            "Epoch: 65, Loss: 2.428452\n",
            "Epoch: 66, Loss: 2.294750\n",
            "Epoch: 67, Loss: 2.350605\n",
            "Epoch: 68, Loss: 2.454967\n",
            "Epoch: 69, Loss: 2.321086\n",
            "Epoch: 70, Loss: 2.385792\n",
            "Epoch: 71, Loss: 2.427301\n",
            "Epoch: 72, Loss: 2.284851\n",
            "Epoch: 73, Loss: 2.323378\n",
            "Epoch: 74, Loss: 2.443795\n",
            "Epoch: 75, Loss: 2.399312\n",
            "Epoch: 76, Loss: 2.312900\n",
            "Epoch: 77, Loss: 2.239473\n",
            "Epoch: 78, Loss: 2.308671\n",
            "Epoch: 79, Loss: 2.288528\n",
            "Epoch: 80, Loss: 2.384051\n",
            "Epoch: 81, Loss: 2.419299\n",
            "Epoch: 82, Loss: 2.382040\n",
            "Epoch: 83, Loss: 2.335288\n",
            "Epoch: 84, Loss: 2.335229\n",
            "Epoch: 85, Loss: 2.402462\n",
            "Epoch: 86, Loss: 2.519231\n",
            "Epoch: 87, Loss: 2.362942\n",
            "Epoch: 88, Loss: 2.416080\n",
            "Epoch: 89, Loss: 2.441981\n",
            "Epoch: 90, Loss: 2.398281\n",
            "Epoch: 91, Loss: 2.282699\n",
            "Epoch: 92, Loss: 2.317730\n",
            "Epoch: 93, Loss: 2.392527\n",
            "Epoch: 94, Loss: 2.355880\n",
            "Epoch: 95, Loss: 2.388875\n",
            "Epoch: 96, Loss: 2.260125\n",
            "Epoch: 97, Loss: 2.340477\n",
            "Epoch: 98, Loss: 2.427576\n",
            "Epoch: 99, Loss: 2.388023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = cifar10_model_extended(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHSzK4QgqlSz",
        "outputId": "4e7b19df-b39f-468a-895d-4cd226343ee5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.094440\n"
          ]
        }
      ]
    }
  ]
}